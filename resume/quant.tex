% https://www.overleaf.com/latex/templates/cis-grad-template-dev/pjcttwysqvym

% Use the custom resume.cls style
\documentclass{resume}

% Document margins
\usepackage[top=0.15in, bottom=0.15in, right=0.4in, left=0.3in]{geometry}
\usepackage{fontawesome}


\name{Wei-Chih Huang}
\address{
\href{mailto:noctildon2@gmail.com}{\faEnvelope\, noctildon2@gmail.com} \\
\href{https://www.linkedin.com/in/wei-chih-huang}{\faLinkedinSquare \,/in/wei-chih-huang} \\
\href{https://github.com/noctildon}{\faGithub \, noctildon} \\
\href{https://noctildon.github.io}{\faDesktop \, Personal Site} \\
\href{https://inspirehep.net/authors/2661451?ui-citation-summary=true}{\faGraduationCap\,Publications}
}


\begin{document}

\vspace{-1.5em}
\begin{rSection}{Education}
\vspace{-0.25em}
{\bf PhD in Physics}, Texas A\&M University, US \hfill {Aug. 2019 - Dec. 2025} \\
{\bf BS in Physics}, National Tsing Hua University, Taiwan \hfill {Aug. 2015 - Jun. 2019}
\end{rSection}


% \vspace{-0.5em}
\begin{rSection}{Experience}
    \vspace{-1.5em}
    \item \textbf{Data scientist internship} - Capital One Auto Finance \hfill Jun. 2025 - Aug. 2025
        % \newline\hspace*{0.5em} {\normalsize}
        \begin{itemize}
        \item Reduced 10\% loss for auto loans with machine learning models (GBM, NN, LSTM) using PyTorch
        \item Achieved 99\% prediction accuracy for future payments and default probability using time-series ML model
        \item Designed customized time series model, training loop and loss function to better aling with business needs
        \item Fetched 10 TB of data from SnowFlake and processed statistical data analysis on AWS
        \item Collaborated with 4 product managers to translate model outputs into action-based decisions
        \end{itemize}

    \vspace{0.5em}
    \item \textbf{Quantitative Researcher/Engineer} - \href{https://www.linkedin.com/company/aggieqf}{Aggie Quant Fund} \hfill Jan. 2024 - Dec. 2024
        \newline\hspace*{0.5em} {\normalsize Application of cutting-edge technologies to financial market}
        \begin{itemize}
        \item Managed \$100,000 fund and developed models for stock forecasting and portfolio optimization
        \item Outperformed S\&P500 by 200\% by AI, sentiment analysis, language model, and alpha research
        \item Saved 70\% time by efficient, automatic and high-performance price database with InfluxDB
        \item With 0 costs extract market insight everyday from finance news with GitHub Actions and cloud LLM
        \item Collaborated in a 10-person team to optimize portfolio, mitigate risks and monitor trades
        \end{itemize}

    \vspace{0.5em}
    \item \textbf{Research Assistant} - Physics Department, Texas A\&M University (\href{https://inspirehep.net/authors/2661451}{researcher profile}) \hfill Aug. 2019 - Jun. 2025
        \newline\hspace*{0.5em} {\normalsize PhD dissertation on high energy dark matter particle search}
        \begin{itemize}
        \item Published 7 papers in high impact journals and presented successful talks at international conferences
        % \item Built physics models and conducted the statistical analysis on 1B rows of multi-dimensional data by Python
        \item Analyzed 1B rows of multi-dimensional high energy particle data
        \item Accelerated the analysis by 1000 times with dedicated algorithm, multiprocessing, caching, and C\texttt{++}
        \item Save 95\% time in data visualization by NumPy, SciPy, Pandas, and Matplotlib
        \item Arranged 20 TB memory and 3000 CPU cores in MPI/OpenMP computer cluster to complete particle simulation
        \item Reduced 90\% of time on high energy particle simulation with machine learning models
        \end{itemize}

    \vspace{0.5em}
    \item \textbf{Independent Data Science Researcher} - {Pro Cyclists Race Analysis} (\href{https://github.com/noctildon/pro_cyclists}{Github repo}) \hfill Apr. 2022 - May. 2024
        \newline\hspace*{0.5em} {\normalsize Data analysis of professional sports and forecast using machine learning models}
        \begin{itemize}
        \item Processed and analyzed 12M rows of data with NumPy, Pandas, SciPy, scikit-learn, and PySpark
        \item Achieved 90\% prediction accuracy using machine learning models with PyTorch and sciki-learn
        \item Saved 80\% cloud bill by deploying data and model to Runpod
        \item Increased 500\% web scraping performance with multi-threading BeautifulSoup
        \end{itemize}

    % \vspace{1em}
    % \item \textbf{Data Science Ambassador} - Physics Department, Texas A\&M University \hfill Aug. 2022 - Aug. 2023
    %     \newline\hspace*{0.5em} {\normalsize Data Science Ambassador representing Physics Department}
    %     \begin{itemize}
    %     \item Provided training and consulting to the department and the students (\href{https://noctildon.github.io/DS_ambassador/index.html}{webpage})
    %     \item Designed interactive workshops on topics including Python, Linux, statistics, data analysis, and machine learning
    %     \end{itemize}

\end{rSection}


\begin{rSection}{Certifications}
    \vspace{-1.2em}
    \item ~~~~\textbullet~~Fundamentals of Accelerated Computing with CUDA C/C\texttt{++} \vspace{-0.5em}
    \item \href{https://www.coursera.org/account/accomplishments/verify/N4T4JR8XLU3Z}{\color{black}{~~~~\textbullet~~Machine Learning Foundations: Algorithmic Foundations}} \vspace{-0.5em}
    \item \href{https://www.coursera.org/account/accomplishments/verify/4X4TD24NFAY4}{\color{black}{~~~~\textbullet~~Machine Learning Foundations: Mathematical Foundations}} \vspace{-0.5em}
    \item \href{https://www.coursera.org/account/accomplishments/verify/TJKKSPN42ZEE}{\color{black}{~~~~\textbullet~~Machine Learning Techniques}} \vspace{-0.5em}
    \item \href{https://www.coursera.org/account/accomplishments/verify/KHCRSS4J4TZE}{\color{black}{~~~~\textbullet~~Divide and Conquer, Sorting and Searching, and Randomized Algorithms}} \vspace{-0.5em}
    \item \href{https://www.coursera.org/account/accomplishments/verify/5B7WBDL9BHVF}{\color{black}{~~~~\textbullet~~A Crash Course in Causality: Inferring Causal Effects from Observational Data}}  \vspace{-0.5em}
\end{rSection}


\end{document}
